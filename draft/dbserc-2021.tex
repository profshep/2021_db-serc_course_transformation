\documentclass[10pt,letterpaper]{article}

\input{preamble}

\begin{document}
\head{Proposal overview and objectives} %<<overview>>

Student-centered pedagogies have been developed and demonstrated to improve student learning. 
For example, in the Process Oriented Guided Inquiry Learning (\pogil) approach, students construct their own knowledge
through a learning cycle of exploration, concept invention, and application.
Students progress through carefully constructed worksheets in small groups to explore a `model' (an information rich data display), answer questions that make them think about the model, propose explanations for what they have explored, and then apply those concepts to further problems.
% 
As successful as I have found \pogil to be, the assessment strategies I have used are still teacher-centered. Typical programs of high stakes assessments fundamentally undermine my learning goals for students. Rather than focusing on growing as a learner and improving as a scholar, traditional grading incentivizes a grade focus in which students try to maximize the points they can get from any assignment for the minimum of effort, independent of their learning. Poor performance on an assessment is met with the attitude that it is too late to do anything about it, so move on to the next chapter. As I result, I hate grading not because it requires effort, but because students pay no attention to the feedback and do not use the feedback as a learning opportunity. 
%work on this
The traditional grading approaches that I have used are a problem because they lead to student frustration, stereotype threat, and attrition from STEM. 

A long range goal of my teaching is to help students embrace a life of intellectual growth and learning both in the subject matter of Chemistry and in the metacognitive and metaemotional skills they need to succeed beyond the Chemistry classroom.
%
The overall objective for this proposal is to develop a proficiency-based assessment structure for General Chemistry 1 that will be transferrable to other instructors in the General Chemistry program. 
% <<hypothesis>> 
The \textit{central hypothesis} is that a proficiency-based grading structure will better encourage and motivate students to identify their weaknesses and work to improve. 
% what would be better?
\fixme{better \begin{enumerate*}[label=\alph*)] \item engagement \item learning outcomes \item attitudes to the subject \item anxiety \item self-efficacy \end{enumerate*}} 
%
This hypothesis is based on the demonstrated effectiveness of specifications grading and standards-based grading in other General Chemistry courses.\cite{Boesdorfer2018,Martin2019}
%<<rationale>>
The \textit{rationale} of this project is that the combination of student-centered pedagogy  and student-centered assessment will synergistically improve learning outcomes for students.  These course materials will be developed with a team of General Chemistry faculty to maximize the transferability of the materials. 
%<<qualifications>>
Our team is uniquely qualified to successfully execute this project because of our experience implementing \pogil in the large-enrollment General Chemistry classroom.

The specific goals for this project are to:
\begin{enumerate}[nosep,label=\textbf{\arabic*}.]
%\begin{itemize}[nosep]
\item \aimtext{aim:grading}{Develop student learning objectives based and align a proficiency-based grading scheme to those objectives.} 
We will design a set of learning objectives  based on Marzano's taxonomy, which provides a structure to specify appropriate learning objectives at multiple levels of conceptual difficulty. The four tiers of achievement -- \Recall, \Comprehension, \Analysis, and \Use -- will be articulated for each learning objective. With this set of learning objectives, we will craft a grading scheme based on a standards-based grading that incorporates both many modes of frequent, low-stakes student work -- online adaptive learning systems (Sapling Learning Curve), online homework (Sapling Learning), individual and group quizzes, discussion boards, \fixme{ other? too much?}.

\item \aimtext{aim:implement}{Design assessments that align to the learning objectives and implement the grading scheme.}
For each learning objective and each level of mastery, assignments will be refactored to best align with the stated learning goals. While many materials exist in the form of question banks, rarely are these questions explicitly aligned with learning objectives. This aim will curate materials like quiz questions and build quizes for each learning unit and design multiple attempts into the assessment delivery system. In addition,  a framework to articulate to students what learning objectives they have mastered and what areas need development will be implemented in the Canvas LMS.

\item \aimtext{aim:assess}{Assess and disseminate the course transformation.}
This aim will measure the impact of the transformation on student learning and attitudes. Based on our previous attitudinal data, we will compare the effect of the course on student self-efficacy and engagement, especially measures of student frustration. In addition, we will measure differential impact on students based on demographic factors such as gender. Women tend to underperform men on high-stakes assessments even though they outperform them at low-stakes assessments. We will measure student performance in the transformed course to see if it alleviates this gender bias.
\end{enumerate}
\myaimref*{aim:grading} will develop a clear, comprehensive set of learning objectives and an associated grading scheme. 
\myaimref*{aim:implement} will develop the actual assessment instruments that will assess student learning and communicate to students their progress. Finally, \myaimref*{aim:assess} will measure how effectively the course transformation has achieved each of its goals and share these results with other faculty.

This project is a \textit{creative} and \textit{original} approach to \fixme{etc etc etc}
%
\oldtext{The \textit{expected outcomes} of this project will be course materials whose effectiveness has been tested. 
%
The development of this project is important at several levels. First, in my classroom, these materials will help my students learn some of the hardest material better. Second, these materials can be distributed to other sections of the same course within the Department of Chemistry. Third, it will serve as a nucleus for further incorporation of active engagement aided by technology in the broader natural sciences community at Pitt. Last, validated materials can be disseminated in the \pogil community and adopted at universities around the country. }
%
Finally, this work engages a question very timely question in the broader scholarship of teaching and learning -- how do we best build student-centered assessments that match student-centered pedagogies? 
\fixme{Barriers to grading reform. expand -- maybe something like how to implement multiple attempts in large-enrollment courses without overwhelming grader resources}

\oldtext{
\head{Expected significance}
This project addresses a central goal of the dB-SERC -- individualizes instruction through interactivity, and personalizes learning through guided inquiry. 
%
%
Finally, the approach embodied in this proposal, once it has been validated, may be generalized to transforming large lecture courses in chemistry and physics.

%

\head{Background and preliminary results}


Aligning instructional activities, evaluation methods, and student learning outcomes is important  

It is useful to use a learning taxonomy when designing learning objectives that have a hierarchical structure. 

Marzano proposed a hierarchical ordering of four cognitive processes -- retrieval, comprehension, analysis, and knowledge utilization. The lowest level, retrieval, focuses on the recognition or recall of information. For example, when a student is asked to define a word or provide a synonym, they demonstrate simple declarative level knowledge. The next level, comprehension, includes integrating and symbolizing. The process of integration involves understanding general relationships related to how the information is organized. For example, a student would be able to describe all variables associated with a model and the subsequent relationships between these variables. Symbolizing requires the learner to translate the knowledge into some nonlinguistic or abstract form like a graph or equation. Processes that extend beyond identifying essential and non essential characteristics of a topic fall in the domain of analysis. An analysis task requires the learner to reorganize the information in a way that generates new conclusions. Marzano's Taxonomy proposes five processes associated with the analysis level -- matching, classifying, analyzing errors, generalizing, and specifying. Finally, the highest cognitive level, knowledge utilization, includes tasks such as decision making, problem solving, experimenting, and investigating. The key distinction from analysis is the focus of the mental activity on a specific situation rather than the actual knowledge itself. For example, when a student analyzes an error involving a gas law relationship the focus is on the gas law. When the student uses a gas law to make a decision regarding whether or not to transport a container of helium gas, the focus is the situation. 
 

Guided inquiry methods~\cite{farrellJCE-99,lewisJCE-05,minderhoutBMBE-07,moog-08,eberleinBMBE-08}, Process Oriented Guided Inquiry Learning~\cite{moog-08} (\pogil) for example, effectively bring active learning into the science classroom. For example, in my thermodynamics and statistical mechanics courses, students first explore a model, then construct the explanation, and then learn the standard terminology for that concept. Guided inquiry methods improve student learning outcomes and attitudes in both introductory and advanced chemistry classrooms~\cite{lewisJCE-05}. 

\subhead{The problem}

Typical guided inquiry materials are worksheets which are \textit{static}, but physical chemistry is \textit{dynamic}. 
Static pictures are poor representations on which to develop an understanding of how molecules move: how molecular collisions create drag but also diffusion; how the kinetic energy of one particle is randomized through collisions to become heat;
how the flow of heat or particles is related to a gradient of temperature or concentration. All of these points are inherently \textit{dynamic}, which is a deep connection to the other objectives of this proposal. Students must employ complex spatiotemporal reasoning (how do things move as a function of time), to connect molecular pictures to macroscopic observables to symbolic representations of those parameters~\cite{burkeJCE-98,marsonJCE-11}. 

\subhead{The innovation}

I have developed electronic course materials (\textsc{html5} applications) for my undergraduate Physical Chemistry courses ($\sim$35 students). The \textsc{html5} applications run on any computer with a modern browser, (Windows, Linux, or Mac), including desktops, laptops, tablets, smartphones, and other app-capable devices. Programs are written in JavaScript, which is slower than C-code by only a factor of 3 for most numerical computation benchmarks~\cite{Khan2014}, which makes it acceptable for scientific computations of this scale. Emscripten and typed arrays offer modest speed-ups~\cite{Khan2014}. 

These computer simulations are \textbf{integrated into guided inquiry materials} and have the potential to radically change the way students engage with the toughest concepts in physical chemistry. Paper worksheets are the proven medium for students to develop their thoughts, compose their sentences, and conduct their calculations. Integrating the computer simulations with the paper record is as straightforward as a Quick Response (QR) code~\myfigref[a]{fig:mockups}. All mobile devices have QR code readers available for free. Simply scanning the QR code takes the browser directly to the webpage of the model. A human readable format is also provided.

Students can interact with the dynamic models~\myfigref[b]{fig:mockups} in real time. As the molecular representation evolves (upper portion of the screen), students can `poke' the simulation and observe how the macroscopic information (bottom of the screen) responds. Three examples \myfigref[b]{fig:mockups} embody concepts that most students find very hard to master, Brownian motion, heat transport, and the structure of liquids.

Sufficient numbers of students have access to smart-phones, tablets, and laptops in class that every \pogil group had access to at least one this past spring semester. The ability to use students' devices in the classroom for learning purposes exists today and will only grow. 

Moving between visualization and class discussion is seamless. Preliminary development of the applications has demonstrated dynamics of $\sim500$ hard spheres in two-dimensions on all platforms in portable code (JavaScript). User interactions include touch, swipe, and multitouch (pinch). These preliminary results show both the technical feasibility of the approach and the viable real-world classroom implementation. The next  tasks are to assess the impact of my implementation of \pogil \myaimref{aim:pogil}, to assess the impact of these new models on student learning and attitudes \myaimref{aim:dynamics}, and to refine the implementation of the models based on the results of the assessments \myaimref{aim:refine}.

\subhead{Assessment strategy}

The efficacy of my implementation of the  \pogil approach will be assessed with multiple methods. Though this is mostly `action research', there is the possibility to share results in papers at \pogil conferences. Therefore,  I will seek IRB approval. 

The general strategy is to use the parallel section of Physical Chemistry 2 as a control population to assess the differences between the \pogil approach and a traditional lecture on student learning and attitudes. Provided that the instructor (TBD) approves, this could be a powerful method of testing differences between pedagogical styles. As much as possible, the relevant assessment tools will be deployed in both classes. 

Nevertheless, there are two important variables to control: population bias and instructor bias. First, \textit{population bias} could affect the quantitative assessments of learning. While student numbers in each section are in the 20--40 range, one section is a morning class and one section is an evening class. To control for the possible bias this could induce, I will use the student QPA which should be a relevant proxy for student ability. The Director of Undergraduate Studies (Bandik) will provide anonymized QPA for each section, which can be used in equivalence testing to test if the sections are similarly populated. Second, \textit{instructor bias} could affect the differences in student attitudes. Many surveys of teaching are influenced by the ``beauty contest'' effect. How much students ``like'' a certain professor can influence the answers they give. Student Assessments of Learning Gains (SALG) instruments are a demonstrated method to elicit specific and actionable feedback from students which separates, as much as possible, the teacher from the teacher's pedagogy~\cite{Seymour2000}. The SALG survey will be a comprehensive assessment of the learning gains in the class, the \pogil method, as well as individual activities (Appendix), and will be one important assessment instrument because it is known to be resistant to instructor bias.
}%end oldtext

%
% AIM Develop grading scheme and learning objectives
%
\aim{aim:grading}
Though the object of teaching is for students to learn,  the assessments used and the grading scheme applied grading often align with those learning objectives in only an intuitive, unexpressed, or traditional way. Clear-headed assessment of student learning should begin with a clear articulation of the learning objectives that is integrated into all aspects of student work and communication with students. This wholistic approach will help students identify the areas that they have mastered and the areas in which  they need to improve.

\subaim{Student Learning Objectives}
Student learning objectives that span the essential content and the hierarchy of cognitive levels from \recall to \use will be refined for the General Chemistry 1 course. Preliminary work has identified 11 major learning objectives and expressed the four mastery levels (Appendix). These objectives will be refined to accommodate consensus among the team of \pogil instructors at Pitt. 

\subaim{Grading scheme}

\subsubaim{Levels of mastery} Four levels of content mastery are \recall, \comprehension, \analysis, and \use. Each level will be worth one point for that learning objective on a Knowledge Assessment. We will examine the feasibility of allowing students to only answer \use questions in Canvas when they have demonstrated master of two or three of the lower levels. 

\begin{itemize}
\item \Recall questions will be, for example, multiple choice questions without significant distractors 
\item \Comprehension
\item \Analysis
\item \Use
\end{itemize}

\subsubaim{Map Progress to Points} We need to decide how achievement at different levels maps onto an actual score. \fixme{This there a way to do this without taking the plunge? Estimate numbers of students at each level to keep similar grade distribution if students did similar quality work? What would we look at to evaluate if it is ok or needs to change?}

An initial approach is given in \cref{tbl:grading_scheme_mockup}.
\begin{wraptable}{R}{0pt}
\begin{tabular}{lr}
&percent\\
% Objective 1 & 4 &\\
% Objective 2 & 4 &\\
% $\vdots$ & &\\
% Objective 11 & 4 &\\ 
Knowledge Assessments & 55\\
Lab  & 15\\
Homework  & 15\\
Discussion board  & 10\\
In-class participation  & *\\ 
ACS Final exam  & 5\\
Extra credit  & *
\end{tabular}
\caption{\label{tbl:grading_scheme_mockup}
The revised point structure will be quite similar to the existing point distribution.} 
\end{wraptable}

We anticipate that the existing scheme for translating course points to letter grade will suffice for the course transformation. We can project how performance on the assessments will affect students' grades using typical values for performance in the other categories. Typical students will accumulate 90\% of homework, 90\% of discussion, 85\% percent lab, and 70\% on the ACS exam. The expected base score would then be 43\%. To pass the course with a C, a target of many biology students, students would need to score  54\% on the assessments, which translates to an average of 2.16 on all assessments, which corresponds slightly better than to achieving proficiency in all \recall and \comprehension areas. To score an A, a student would need to score 3.6 on all assessments, which corresponds to achieving proficiency in all \recall, \comprehension, and \analysis areas as well as $\sim60$\% of \use questions. 

\begin{wraptable}{R}{0pt}
\begin{tabular}{ll}
percent&Grade\\
93--99&A\\
90--92&A-\\
87--89&B+\\
83--86&B\\
80--82&B-\\
77--79&C+\\
73--76&C\\
70--72&C-\\
67--69&D+\\
63--66&D\\
60--62&D-\\
0--59&F
\end{tabular}
\caption{\label{tbl:points}
We anticipate that the scoring structure will remain unchanged.} 
\end{wraptable}


\subsubaim{Build Grading Structure in Canvas}
\fixme{We need to figure out a reasonable way to do this in Canvas. I am not seeing it right now. Options include \begin{enumerate*}[label=\textbf{\alph*)}]\item aligning to outcomes, \item quizzes with multiple attempts, \item an assignment group per learning objective, \item modules for each objective \item mastery paths\end{enumerate*}}

%
% AIM Implementation
%
\aim{aim:implement}

\subaim{Evaluate, revise, and write assessments}
\subsubaim{Evaluate} Existing assessments will first be sorted by their learning outcome (subject area and cognitive level). 

\subsubaim{Revise} Questions that are similar enough to explicit learning objectives will be revised and 

\subsubaim{Devise} New questions will be written and put into Item Banks for sharing.

\subaim{Assemble assessments}
All assessments will be drafted for the term. The assessments will include banks of questions so students see new questions on each attempt. \fixme{Can we give some estimates of what this would be? 4 levels, 11 areas,  3 attempts, }

\subaim{Implement integration Canvas}

\subaim{}

%\subaim{Expected outcomes}

%\subaim{Potential problems and alternative approaches}

%
% AIM Assess and disseminate
%
\aim{aim:assess}
\oldtext{some kind of introduction}

\subaim{Assess Student Learning}

\subsubaim{Course content} Compare to 2018-2021 data for assessment items.

\subsubaim{ACS exam} ACS Gen Chem 1 Paired Question exam. \fixme{[add to budget]} Compare to 2018, 2019 ACS exam data. Scores for POGIL sections.

\subsubaim{Follow on performance} Track students going into Gen Chem 2 with other instructors. 

%http://chemexams.chem.iastate.edu/instructors/assessment-materials/exams
\subsubaim{Student learning self-assessment}
Student perception of learning gains will be assessed through an instrument developed through \url{salgsite.org}, which provides easy implementation and analysis of the questionnaires. 

The question areas relevant to course learning will include
\begin{enumerate*}[label=\textbf{\arabic*.)}]
  \item how do in-class activities support learning,
\item how does assessment structure support learning.
\end{enumerate*}

\subsubaim{Demographic effects} We will compare student performance sorted by demographic category and compare against prior years. Gender performance gaps have been identified in STEM classrooms at Pitt and suggested to be present in our Chemistry program. We will identify the magnitude of the gender performance gap in our previous General Chemistry classes 2018--2021 and compare this with student performance after the course transformation.

\subaim{Assess Student Attitudes}
We will reimplement the surveys of student opinions used in the 2018-2019 terms that captured student.
We will also capture data from concurrent \pogil and traditional lecture courses. Instructors will be encouraged to offer students extra credit or some reward for participation.

\subsubaim{Engagement} 
``I feel engaged during class time.'' ``I do not feel engaged during class time.'' ``I feel engaged when I am working on my homework or out of class assignments.'' ``I do not feel engaged when I am working on my homework or out of class assignments.''


\subsubaim{Self-efficacy}
The attitude surveys will include questions to assess student perceptions of their self-efficacy in postively coded questions like, ``When I face a hard problem, I feel like I can solve it,'' and negatively coded questions like, ``I get frustrated easily when I face a challenging problem.'' These showed a meaningful shift after the implementation of \pogil  in the large-enrollment course compared to traditional lecture.

\subsubaim{Identity}
``I am a science kind of person.'' ``I am a biology kind of person.'' ``I am a chemistry kind of person.'' ``I am a physics kind of person.'' \fixme{TODO: review previous results from Paulette.}

\subsubaim{Anxiety}
``Taking the assessments (quizzes and exams) was stressful.'' ``I felt relaxed completing assessments (quizzes and exams).'' ``I am worried about my performance on my next assessment (quiz or exam).'' ``I feel confident about my next assessment (quiz or exam).''

\subsubaim{Focus groups} In addition to these quantitative surveys, qualitative data will be obtained in focus group sessions. The sessions will elicit feedback on the effectiveness of the assessment approach in the course. Sessions will be voluntary and occur at three points in the semester.

\subaim{Disseminate to Other Chemistry Faculty}
\oldtext{

Currently three activities have been developed and deployed \myfigref{fig:mockups}. The student reaction was positive. Nevertheless, the activities clustered at the end of the semester. Based on this initial success, the activities will be expanded both to support more of the learning goals of the class, as well as to spread the simulations more evenly through the semester. New simulations will include 
\begin{enumerate*}[label=\textbf{\arabic*)}]
\item the dynamical approach to equilibrium (in the first week of class);
\item the flow of heat in systems described by a discrete energy-level diagram (before the first midterm exam); and
\item the thermodynamic cycle of an engine (before the second midterm).
\end{enumerate*}
These three new simulations will meet the needs revealed by student misconceptions in this past semester.


\head{Expected outcomes}

The three aims will together provide a route to demonstrably effective course materials. \myaimref*{aim:pogil} provides the effectiveness of the \pogil approach for this particular course. In that context, \myaimref*{aim:dynamics} provides a comprehensive assessment of the strengths and weaknesses of the dynamic course materials I have developed. \myaimref*{aim:refine} provides a pathway to incorporate the gained understanding into improved materials.

\subhead{Potential problems and alternative approaches}

Though unlikely, it is possible that the instructor of the parallel section of Physical Chemistry 2 will not agree to cooperate. This problem could eliminate the control population. Nevertheless, the diverse portfolio of student feedback I plan to collect is not exclusively dependent on the ability to make the case and control comparison for quiz and exam questions. A meaningful data-set should still be possible given the student self-assessment of learning gains and student feedback on the effectiveness of the dynamic displays to aid their learning.

Another potential problem, is that the student populations in the two courses may indeed be different. Recent experience in the course suggests that both courses have roughly the same proportions of A's and B's, nevertheless, the equivalence tests may fail in this year. Having unequal populations means one cannot say this study \text{proves} that \pogil is better in an absolute sense, but that is not the goal of this project. The differences in learning gains and attitude shifts should still be perceptible, and the development and refinement of course materials will still be valuable.

\head{Timeline and Budget justification}

The funds from this award will go to dedicated time during summer 2015 to prepare the new simulations and the assessment tools (surveys, questionnaires, rubrics for marking free-response questions, and focus group preparation) (0.5 month summer salary). The project will support two graduate students (1 month effort each). I will mentor the graduate students in evidence-based teaching and assessment methods. The students will then get hands-on experience designing and evaluating the pre- and post-assessments for these interventions and participating in a focus group session (Spring of 2016). An undergraduate programmer will revise and enhance the simulations in Fall 2015 (\$10 / hour, 10 hours / week, 15 weeks).
}%end oldtext
\newpage
\raggedright\footnotesize\singlespacing
\renewcommand{\refname}{\large\textbf{References}}
%\renewcommand{\refname}{\Large\textsc{\textbf{\lsstyle \MakeTextLowercase References}}}
\bibliography{library_fixed,library_extra}
%trick to not typeset the bibliogrpahy but make all the data
%{\setbox0\vbox{\bibliography{library,sgr,teaching}}}

% \newpage
% \includepdf[pages={1-},offset=75 -75]{dB-SERC_BUDGET.pdf}

% \newpage
% \includepdf[pages={1-},offset=75 -75]{dBSERC_support_letter_Garrett-Roe_2015.pdf}


\end{document}