\documentclass[10pt,letterpaper]{article}

\input{preamble}

\begin{document}
\head{Proposal overview and objectives} %<<overview>>

Student-centered pedagogies have been developed and demonstrated to improve student learning. 
For example, in the Process Oriented Guided Inquiry Learning (\pogil) approach, students construct their own knowledge
through a learning cycle of exploration, concept invention, and application.
Students progress through carefully constructed worksheets in small groups to explore a `model' (an information rich data display), answer questions that make them think about the model, propose explanations for what they have explored, and then apply those concepts to further problems.
% 
As successful as I have found \pogil to be, the assessment strategies I have used are still teacher-centered. Typical programs of high stakes assessments fundamentally undermine my learning goals for students. Rather than focusing on growing as a learner and improving as a scholar, traditional grading incentivizes a grade focus in which students try to maximize the points they can get from any assignment for the minimum of effort, independent of their learning. Poor performance on an assessment is met with the attitude that it is too late to do anything about it, so move on to the next chapter. As I result, I hate grading not because it requires effort, but because students pay no attention to the feedback and do not use the feedback as a learning opportunity. 
%work on this
The traditional grading approaches that I have used are a problem because they lead to student frustration, stereotype threat, and attrition from STEM. 

A long range goal of my teaching is to help students embrace a life of intellectual growth and learning both in the subject matter of Chemistry and in the metacognitive and metaemotional skills they need to succeed beyond the Chemistry classroom.
%
The overall objective for this proposal is to develop a proficiency-based assessment structure for General Chemistry 1 that will be transferrable to other instructors in the General Chemistry program. 
% <<hypothesis>> 
The \textit{central hypothesis} is that a proficiency-based grading structure will better encourage and motivate students to identify their weaknesses and work to improve. 
% what would be better?
\fixme{better \begin{enumerate*}[label=\alph*)] \item engagement \item learning outcomes \item attitudes to the subject \item anxiety \item self-efficacy \end{enumerate*}} 
%
This hypothesis is based on the demonstrated effectiveness of specifications grading and standards-based grading in other General Chemistry courses.\cite{Boesdorfer2018,Martin2019}
%<<rationale>>
The \textit{rationale} of this project is that the combination of student-centered pedagogy  and student-centered assessment will synergistically improve learning outcomes for students.  These course materials will be developed with a team of General Chemistry faculty to maximize the transferability of the materials. 
%<<qualifications>>
Our team is uniquely qualified to successfully execute this project because of our experience implementing \pogil in the large-enrollment General Chemistry classroom.

The specific goals for this project are to:
\begin{enumerate}[nosep,label=\textbf{\arabic*}.]
%\begin{itemize}[nosep]
\item \aimtext{aim:grading}{Develop student learning objectives based and align a proficiency-based grading scheme to those objectives.} 
We will design a set of learning objectives  based on Marzano's taxonomy, which provides a structure to specify appropriate learning objectives at multiple levels of conceptual difficulty. The four tiers of achievement -- \Recall, \Comprehension, \Analysis, and \Use -- will be articulated for each learning objective. With this set of learning objectives, we will craft a grading scheme based on a standards-based grading that incorporates both many modes of frequent, low-stakes student work -- online adaptive learning systems (Sapling Learning Curve), online homework (Sapling Learning), individual and group quizzes, discussion boards, \fixme{ other? too much?}.

\item \aimtext{aim:implement}{Design assessments that align to the learning objectives and implement the grading scheme.}
For each learning objective and each level of mastery, assignments will be refactored to best align with the stated learning goals. While many materials exist in the form of question banks, rarely are these questions explicitly aligned with learning objectives. This aim will curate materials like quiz questions and build quizes for each learning unit and design multiple attempts into the assessment delivery system. In addition,  a framework to articulate to students what learning objectives they have mastered and what areas need development will be implemented in the Canvas LMS.

\item \aimtext{aim:assess}{Assess and disseminate the course transformation.}
This aim will measure the impact of the transformation on student learning and attitudes. Based on our previous attitudinal data, we will compare the effect of the course on student self-efficacy and engagement, especially measures of student frustration. In addition, we will measure differential impact on students based on demographic factors such as gender. Women tend to underperform men on high-stakes assessments even though they outperform them at low-stakes assessments. We will measure student performance in the transformed course to see if it alleviates this gender bias.
\end{enumerate}
\myaimref*{aim:grading} will develop a clear, comprehensive set of learning objectives and an associated grading scheme. 
\myaimref*{aim:implement} will develop the actual assessment instruments that will assess student learning and communicate to students their progress. Finally, \myaimref*{aim:assess} will measure how effectively the course transformation has achieved each of its goals and share these results with other faculty.

This project is a \textit{creative} and \textit{original} approach to grading reform in the context of large-enrollment courses.
%
The \textit{expected outcomes} of this project will be course materials -- learning objectives, learning assessments, and course infrastructure -- whose effectiveness has been tested and is ready to be shared. 
%
\oldtext{The development of this project is important at several levels. First, in my classroom, these materials will help my students learn some of the hardest material better. Second, these materials can be distributed to other sections of the same course within the Department of Chemistry. Third, it will serve as a nucleus for further incorporation of active engagement aided by technology in the broader natural sciences community at Pitt. Last, validated materials can be disseminated in the \pogil community and adopted at universities around the country. }
%
Finally, this work engages a question very timely question in the broader scholarship of teaching and learning -- how do we best build student-centered assessments that match student-centered pedagogies? 
\fixme{Barriers to grading reform. expand -- maybe something like how to implement multiple attempts in large-enrollment courses without overwhelming grader resources}

\head{Expected significance}
This course transformation has the potential to affect many hundreds of Pitt undergraduates each term. If this transformation proves useful, other faculty in the General Chemistry teaching pool can adopt this approach beginning with the materials we will develop and share.  The transformation may also inspire other efforts to reform grading policies in other STEM disciplines through the dB-SERC community. Finally, the results of the course transformation will be disseminated in the national \pogil community.

\head{Background and preliminary results}
Guided inquiry methods~\cite{farrellJCE-99,lewisJCE-05,minderhoutBMBE-07,moog-08,eberleinBMBE-08} and Process Oriented Guided Inquiry Learning~\cite{moog-08} (\pogil) in specific, effectively bring active learning into the science classroom. Since 2012, Garrett-Roe has implemented \pogil in upper-level Physical Chemistry classes and, since 2018, in large-enrollment General Chemistry classes.\cite{Vincent-Ruz2020} As such, the \pogil pedagogy is already well-established. On the other hand, grading reform is a much newer development and a variety of approaches have been adopted. We here introduce the learning taxonomy that guides our proposed grading scheme, and introduce related grading schemes that have been demonstrated to be efficacious.

\subhead{A hierarchical taxonomy for student learning objectives}
 We are basing our  proficiency-based grading scheme on a learning taxonomy with a hierarchical structure because that hierarchy more clearly articulates which tasks must be completed before progress to higher levels. This taxonomy is, naturally, very similar to Bloom's taxonomy, but we will highlight how this taxonomy helps make good choices for question design.


Marzano proposed a hierarchical ordering of four cognitive processes -- retrieval, comprehension, analysis, and knowledge utilization. The lowest level, retrieval, focuses on the recognition or recall of information. For example, when a student is asked to define a word or provide a synonym, they demonstrate simple declarative level knowledge. The next level, comprehension, includes integrating and symbolizing. The process of integration involves understanding general relationships related to how the information is organized. For example, a student would be able to describe all variables associated with a model and the subsequent relationships between these variables. Symbolizing requires the learner to translate the knowledge into some nonlinguistic or abstract form like a graph or equation. Processes that extend beyond identifying essential and non essential characteristics of a topic fall in the domain of analysis. An analysis task requires the learner to reorganize the information in a way that generates new conclusions. Marzano's Taxonomy proposes five processes associated with the analysis level -- matching, classifying, analyzing errors, generalizing, and specifying. Finally, the highest cognitive level, knowledge utilization, includes tasks such as decision making, problem solving, experimenting, and investigating. The key distinction from analysis is the focus of the mental activity on a specific situation rather than the actual knowledge itself. For example, when a student analyzes an error involving a gas law relationship the focus is on the gas law. When the student uses a gas law to make a decision regarding whether or not to transport a container of helium gas, the focus is the situation. 
 

\oldtext{
\subhead{The problem}

Typical guided inquiry materials are worksheets which are \textit{static}, but physical chemistry is \textit{dynamic}. 
Static pictures are poor representations on which to develop an understanding of how molecules move: how molecular collisions create drag but also diffusion; how the kinetic energy of one particle is randomized through collisions to become heat;
how the flow of heat or particles is related to a gradient of temperature or concentration. All of these points are inherently \textit{dynamic}, which is a deep connection to the other objectives of this proposal. Students must employ complex spatiotemporal reasoning (how do things move as a function of time), to connect molecular pictures to macroscopic observables to symbolic representations of those parameters~\cite{burkeJCE-98,marsonJCE-11}. 

\subhead{The innovation}

I have developed electronic course materials (\textsc{html5} applications) for my undergraduate Physical Chemistry courses ($\sim$35 students). The \textsc{html5} applications run on any computer with a modern browser, (Windows, Linux, or Mac), including desktops, laptops, tablets, smartphones, and other app-capable devices. Programs are written in JavaScript, which is slower than C-code by only a factor of 3 for most numerical computation benchmarks~\cite{Khan2014}, which makes it acceptable for scientific computations of this scale. Emscripten and typed arrays offer modest speed-ups~\cite{Khan2014}. 

These computer simulations are \textbf{integrated into guided inquiry materials} and have the potential to radically change the way students engage with the toughest concepts in physical chemistry. Paper worksheets are the proven medium for students to develop their thoughts, compose their sentences, and conduct their calculations. Integrating the computer simulations with the paper record is as straightforward as a Quick Response (QR) code~\myfigref[a]{fig:mockups}. All mobile devices have QR code readers available for free. Simply scanning the QR code takes the browser directly to the webpage of the model. A human readable format is also provided.

Students can interact with the dynamic models~\myfigref[b]{fig:mockups} in real time. As the molecular representation evolves (upper portion of the screen), students can `poke' the simulation and observe how the macroscopic information (bottom of the screen) responds. Three examples \myfigref[b]{fig:mockups} embody concepts that most students find very hard to master, Brownian motion, heat transport, and the structure of liquids.

Sufficient numbers of students have access to smart-phones, tablets, and laptops in class that every \pogil group had access to at least one this past spring semester. The ability to use students' devices in the classroom for learning purposes exists today and will only grow. 

Moving between visualization and class discussion is seamless. Preliminary development of the applications has demonstrated dynamics of $\sim500$ hard spheres in two-dimensions on all platforms in portable code (JavaScript). User interactions include touch, swipe, and multitouch (pinch). These preliminary results show both the technical feasibility of the approach and the viable real-world classroom implementation. The next  tasks are to assess the impact of my implementation of \pogil \myaimref{aim:pogil}, to assess the impact of these new models on student learning and attitudes \myaimref{aim:dynamics}, and to refine the implementation of the models based on the results of the assessments \myaimref{aim:refine}.

\subhead{Assessment strategy}

The efficacy of my implementation of the  \pogil approach will be assessed with multiple methods. Though this is mostly `action research', there is the possibility to share results in papers at \pogil conferences. Therefore,  I will seek IRB approval. 

The general strategy is to use the parallel section of Physical Chemistry 2 as a control population to assess the differences between the \pogil approach and a traditional lecture on student learning and attitudes. Provided that the instructor (TBD) approves, this could be a powerful method of testing differences between pedagogical styles. As much as possible, the relevant assessment tools will be deployed in both classes. 

Nevertheless, there are two important variables to control: population bias and instructor bias. First, \textit{population bias} could affect the quantitative assessments of learning. While student numbers in each section are in the 20--40 range, one section is a morning class and one section is an evening class. To control for the possible bias this could induce, I will use the student QPA which should be a relevant proxy for student ability. The Director of Undergraduate Studies (Bandik) will provide anonymized QPA for each section, which can be used in equivalence testing to test if the sections are similarly populated. Second, \textit{instructor bias} could affect the differences in student attitudes. Many surveys of teaching are influenced by the ``beauty contest'' effect. How much students ``like'' a certain professor can influence the answers they give. Student Assessments of Learning Gains (SALG) instruments are a demonstrated method to elicit specific and actionable feedback from students which separates, as much as possible, the teacher from the teacher's pedagogy~\cite{Seymour2000}. The SALG survey will be a comprehensive assessment of the learning gains in the class, the \pogil method, as well as individual activities (Appendix), and will be one important assessment instrument because it is known to be resistant to instructor bias.
}%end oldtext

\fixme{Somewhere we need an estimate of grader time.}

%
% AIM Develop grading scheme and learning objectives
%
\aim{aim:grading}
Though the object of teaching is for students to learn,  the assessments used and the grading scheme applied grading often align with those learning objectives in only an intuitive, unexpressed, or traditional way. Clear-headed assessment of student learning should begin with a clear articulation of the learning objectives that is integrated into all aspects of student work and communication with students. This wholistic approach will help students identify the areas that they have mastered and the areas in which  they need to improve.

\subaim{Student Learning Objectives}
Student learning objectives that span the essential content and the hierarchy of cognitive levels from \recall to \use will be refined for the General Chemistry 1 course. Preliminary work has identified 11 major learning objectives and expressed the four mastery levels (Appendix). These objectives will be refined to accommodate consensus among the team of \pogil instructors at Pitt. 

\subaim{Grading scheme}

\subsubaim{Levels of mastery} Four levels of content mastery are \recall, \comprehension, \analysis, and \use. Each level will be worth one point for that learning objective on a Knowledge Assessment. We will examine the feasibility of allowing students to only answer \use questions in Canvas when they have demonstrated master of two or three of the lower levels. 

\begin{itemize}
\item \Recall questions test \fixme{YYY}. For example, multiple choice, sorting, and multiple selection questions without significant distractors are at the \recall level. These questions likely can be computer graded. We consider calculations that are algorithmic also to be at the \recall level. For example,  
\item \Comprehension questions test for the ability to \fixme{XXX}. Multiple choice, sorting,  and multiple selection questions in the presence of  significant distractors are at the \comprehension level and can be computer graded. Selection of the correct answer in the context of significant distractors that express common misconceptions indicates \comprehension. Additional questions such as explaining and drawing will likely require human graders.
\item \Analysis questions test \fixme{ZZZ}. Many free-response calculations fall into this category. Free response numerical result questions can be computer graded. Additional proficiency in the presence of minor errors can be noted by students uploading their work and human graders regrading the response.
\item \Use questions ask students to extend beyond \analysis and put their knowledge to work for their own goals. \Use questions involve steps of deciding, choosing, proposing. We anticipate that human grading will be required for all of these student responses.
\end{itemize}
Examples of questions at each of these levels are provided (Appendix).
%
\fixme{What level is a multiple choice calculation with significant distractors?}

\subsubaim{Map Progress to Points} We need to decide how achievement at different levels maps onto an actual score. \fixme{This there a way to do this without taking the plunge? Estimate numbers of students at each level to keep similar grade distribution if students did similar quality work? What would we look at to evaluate if it is ok or needs to change?}

An initial approach is given in \cref{tbl:grading_scheme_mockup}.
\begin{wraptable}{R}{0pt}
\begin{tabular}{lr}
&percent\\
% Objective 1 & 4 &\\
% Objective 2 & 4 &\\
% $\vdots$ & &\\
% Objective 11 & 4 &\\ 
Knowledge Assessments & 55\\
Lab  & 15\\
Homework  & 15\\
Discussion board  & 10\\
In-class participation  & *\\ 
ACS Final exam  & 5\\
Extra credit  & *
\end{tabular}
\caption{\label{tbl:grading_scheme_mockup}
The revised point structure will be quite similar to the existing point distribution.} 
\end{wraptable}

We anticipate that the existing scheme for translating course points to letter grade will suffice for the course transformation. We can project how performance on the assessments will affect students' grades using typical values for performance in the other categories. Typical students will accumulate 90\% of homework, 90\% of discussion, 85\% percent lab, and 70\% on the ACS exam. The expected base score would then be 43\%. To pass the course with a C, a target of many biology students, students would need to score  54\% on the assessments, which translates to an average of 2.16 on all assessments, which corresponds slightly better than to achieving proficiency in all \recall and \comprehension areas. To score an A, a student would need to score 3.6 on all assessments, which corresponds to achieving proficiency in all \recall, \comprehension, and \analysis areas as well as $\sim60$\% of \use questions. 

\begin{wraptable}{R}{0pt}
\begin{tabular}{ll}
percent&Grade\\
93--99&A\\
90--92&A-\\
87--89&B+\\
83--86&B\\
80--82&B-\\
77--79&C+\\
73--76&C\\
70--72&C-\\
67--69&D+\\
63--66&D\\
60--62&D-\\
0--59&F
\end{tabular}
\caption{\label{tbl:points}
We anticipate that the scoring structure will remain unchanged.} 
\end{wraptable}


\subsubaim{Build Grading Structure in Canvas}
\fixme{We need to figure out a reasonable way to do this in Canvas. I am not seeing it right now. Options include \begin{enumerate*}[label=\textbf{\alph*)}]\item aligning to outcomes, \item quizzes with multiple attempts, \item an assignment group per learning objective, \item modules for each objective \item mastery paths\end{enumerate*}}

%
% AIM Implementation
%
\aim{aim:implement}

\subaim{Evaluate, revise, and write assessments}
\subsubaim{Grow a taxonomically organized question bank}\label{ssa:question_bank} Existing assessments will first be sorted by their learning outcome (subject area and cognitive level). We have a library of \fixme{131} questions that been designed in accordance with this grading scheme for human grading (Shepherd) and $\sim200$ unsorted questions that have been implemented in Canvas for automatic grading (Garrett-Roe). The pool of unsorted questions will be evaluated, grouped according to their learning level, and revised as necessary. Additional questions will be written to fill any identified gaps. We aim to develop question banks of $\sim6$ questions per learning objective (264 questions total). 

% \subsubaim{Revise} Questions that are similar enough to explicit learning objectives will be revised and 

% \subsubaim{Devise} New questions will be written and put into Item Banks for sharing.

\subaim{Assemble assessments}
All assessments will be drafted for the term based on the item banks developed in \myssaref{ssa:question_bank}. Each assessment will contain one question, drawn from the item banks, at each cognitive level. Item banks of 6 questions will provide a reasonable likelihood that students will see new questions on each attempt.

\subaim{Implement integration canvas}
The grade book infrastructure will be developed to calculate grades according to the grading scheme and to efficiently communicate to students their current learning progress. Potential approaches to be explored are quizzes with multiple attempts, multiple assignments with grade pooling, alignment with the Learning Objectives feature, and Mastery Paths in content Modules. We will select, from these options, the approach that offers the clearest implementation from the instructor perspective and the clearest communication to the student. Appropriate syllabus text will be developed and shared.
 
\subaim{Deploy the course transformation}
In Fall 2021, the transformed course will be implemented. Each week, students will complete an assessment with four learning goals. Every three weeks students will have another opportunity to repeat earlier assessments.

%\subaim{Expected outcomes}

%\subaim{Potential problems and alternative approaches}

%
% AIM Assess and disseminate
%
\aim{aim:assess}
\oldtext{some kind of introduction}

\subaim{Assess Student Learning}

\subsubaim{Course content} Compare to 2018-2021 data for assessment items.

\subsubaim{ACS exam} ACS Gen Chem 1 Paired Question exam. \fixme{[add to budget]} Compare to 2018, 2019 ACS exam data. Scores for POGIL sections.

\subsubaim{Follow on performance} Track students going into Gen Chem 2 with other instructors. 

%http://chemexams.chem.iastate.edu/instructors/assessment-materials/exams
\subsubaim{Student learning self-assessment}
Student perception of learning gains will be assessed through an instrument developed through \url{salgsite.org}, which provides easy implementation and analysis of the questionnaires. 

The question areas relevant to course learning will include
\begin{enumerate*}[label=\textbf{\arabic*.)}]
  \item how do in-class activities support learning,
\item how does assessment structure support learning.
\end{enumerate*}

\subsubaim{Demographic effects} We will compare student performance sorted by demographic category and compare against prior years. Gender performance gaps have been identified in STEM classrooms at Pitt and suggested to be present in our Chemistry program. We will identify the magnitude of the gender performance gap in our previous General Chemistry classes 2018--2021 and compare this with student performance after the course transformation.

\subaim{Assess Student Attitudes}
We will reimplement the surveys of student opinions used in the 2018-2019 terms that captured student.
We will also capture data from concurrent \pogil and traditional lecture courses. Instructors will be encouraged to offer students extra credit or some reward for participation.

\subsubaim{Engagement} 
``I feel engaged during class time.'' ``I do not feel engaged during class time.'' ``I feel engaged when I am working on my homework or out of class assignments.'' ``I do not feel engaged when I am working on my homework or out of class assignments.''


\subsubaim{Self-efficacy}
The attitude surveys will include questions to assess student perceptions of their self-efficacy in postively coded questions like, ``When I face a hard problem, I feel like I can solve it,'' and negatively coded questions like, ``I get frustrated easily when I face a challenging problem.'' These showed a meaningful shift after the implementation of \pogil  in the large-enrollment course compared to traditional lecture.

\subsubaim{Identity}
``I am a science kind of person.'' ``I am a biology kind of person.'' ``I am a chemistry kind of person.'' ``I am a physics kind of person.'' \fixme{TODO: review previous results from Paulette.}

\subsubaim{Anxiety}
``Taking the assessments (quizzes and exams) was stressful.'' ``I felt relaxed completing assessments (quizzes and exams).'' ``I am worried about my performance on my next assessment (quiz or exam).'' ``I feel confident about my next assessment (quiz or exam).''

\subsubaim{Focus groups} In addition to these quantitative surveys, qualitative data will be obtained in focus group sessions. The sessions will elicit feedback on the effectiveness of the assessment approach in the course. Sessions will be voluntary and occur at three points in the semester.

\subaim{Disseminate to Other Chemistry Faculty}
%\subaim{Feedback from \pogil faculty} 
At the beginning and end of the term of work, the \pogil Chemistry faculty (Madison, Meyer, Garrett-Roe) will meet to review the plan for the transformation and provide input into the project design. When approximately half of the course assessment material has been revised, the \pogil faculty will, as a team, review the status of the project. Feedback will be incorporated. At the end of the project, Madison and Meyer will be able to choose if they will also adopt the class transformation at this point or if Garrett-Roe will run a year as a pilot program. During the summer term, the transformation will be presented to other faculty in the General Chemistry program.

\oldtext{

\head{Expected outcomes}

The three aims will together provide a route to demonstrably effective course materials. \myaimref*{aim:pogil} provides the effectiveness of the \pogil approach for this particular course. In that context, \myaimref*{aim:dynamics} provides a comprehensive assessment of the strengths and weaknesses of the dynamic course materials I have developed. \myaimref*{aim:refine} provides a pathway to incorporate the gained understanding into improved materials.

\subhead{Potential problems and alternative approaches}

Though unlikely, it is possible that the instructor of the parallel section of Physical Chemistry 2 will not agree to cooperate. This problem could eliminate the control population. Nevertheless, the diverse portfolio of student feedback I plan to collect is not exclusively dependent on the ability to make the case and control comparison for quiz and exam questions. A meaningful data-set should still be possible given the student self-assessment of learning gains and student feedback on the effectiveness of the dynamic displays to aid their learning.

Another potential problem, is that the student populations in the two courses may indeed be different. Recent experience in the course suggests that both courses have roughly the same proportions of A's and B's, nevertheless, the equivalence tests may fail in this year. Having unequal populations means one cannot say this study \text{proves} that \pogil is better in an absolute sense, but that is not the goal of this project. The differences in learning gains and attitude shifts should still be perceptible, and the development and refinement of course materials will still be valuable.

\head{Timeline and Budget justification}

The funds from this award will go to dedicated time during summer 2015 to prepare the new simulations and the assessment tools (surveys, questionnaires, rubrics for marking free-response questions, and focus group preparation) (0.5 month summer salary). The project will support two graduate students (1 month effort each). I will mentor the graduate students in evidence-based teaching and assessment methods. The students will then get hands-on experience designing and evaluating the pre- and post-assessments for these interventions and participating in a focus group session (Spring of 2016). An undergraduate programmer will revise and enhance the simulations in Fall 2015 (\$10 / hour, 10 hours / week, 15 weeks).
}%end oldtext

\head{Appendices}
List of Appendices
\begin{enumerate}
\item \hyperref[app:learning_objectives]{Example student learning goals for General Chemistry 1 covering both subject area and level of mastery} (\autopageref{app:learning_objectives})
\item \hyperref[app:example_questions]{Examples of questions at each level of mastery} (\autopageref{app:example_questions})
\item \hyperref[app:shepherd_biosketch]{Senior staff biosketch (Shepherd)} (\autopageref{app:shepherd_biosketch})
\end{enumerate}

\newpage
\raggedright\footnotesize\singlespacing
\renewcommand{\refname}{\large\textbf{References}}
%\renewcommand{\refname}{\Large\textsc{\textbf{\lsstyle \MakeTextLowercase References}}}
\bibliography{library_fixed,library_extra}
%trick to not typeset the bibliogrpahy but make all the data
%{\setbox0\vbox{\bibliography{library,sgr,teaching}}}

% \newpage
% \includepdf[pages={1-},offset=75 -75]{dB-SERC_BUDGET.pdf}

% \newpage
% \includepdf[pages={1-},offset=75 -75]{dBSERC_support_letter_Garrett-Roe_2015.pdf}

\newpage
\phantomsection\label{app:learning_objectives}
\fixme{PDF will go here}

\newpage
\phantomsection\label{app:example_questions}
\fixme{PDF will go here}

\newpage
\phantomsection\label{app:shepherd_biosketch}
\fixme{PDF will go here}


\end{document}